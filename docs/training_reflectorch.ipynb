{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a reflectorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary methods from the `reflectorch` package, as well as othar basic Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from reflectorch import SAVED_MODELS_DIR, SaveBestModel, StepLR, get_trainer_by_name, get_callbacks_by_name\n",
    "from reflectorch.extensions.jupyter import JPlotLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    ":class: dropdown\n",
    "\n",
    "Alternatively, we can import everything from reflectorch with\n",
    "`from reflectorch import *`\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a model we use the `Trainer` class, which contains all the components of the training process such as the data generator, the neural network and the optimizer.\n",
    "\n",
    " We can initialize the trainer according to the specifications defined in a YAML configuration file using the `get_trainer_by_name` method which takes as input the name of the configuration file. If the package was installed in editable model, the configuration files are read from the `configs` directory located inside the repository, otherwise the path for the directory containing the configuration file should also be specified using the `config_dir` argument. The `load_weights` argument should be set to `False` since we want the neural network weights to be randomly initialized for a fresh training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model c1.yaml loaded. Number of parameters: 13.37 M\n"
     ]
    }
   ],
   "source": [
    "config_name = 'c1.yaml'\n",
    "trainer = get_trainer_by_name(config_name, load_weights=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The trainer contains several important attributes we can inspect:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. The Pytorch optimizer. We can observe that the optimizer specified in the configuration is AdamW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: [0.9, 0.998]\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0.0005\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    ":::{note}\n",
    "The learning rate can be easily changed using `trainer.set_lr(new_lr)`\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "  2. The batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  3. The Pytorch neural network module. We see that the network is an instance of the class `SubPriorConvFCEncoder_V2`. This architecture consists of a multilayer perceptron (MLP) with residual connections, batch normalization layers and GELU activations (`trainer.model.fc`). A 1D CNN embedding network (`trainer.model.conv`) produces a latent embedding of the input batch of reflectivity curves which is concatenated with the prior bounds for the thin film parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubPriorConvFCEncoder_V2(\n",
       "  (conv): ConvEncoder(\n",
       "    (core): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv1d(1, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (avpool): AdaptiveAvgPool1d(output_size=1)\n",
       "    (fc): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc): ResidualNet(\n",
       "    (initial_layer): Linear(in_features=144, out_features=1024, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x ResidualBlock(\n",
       "        (activation): GELU(approximate='none')\n",
       "        (batch_norm_layers): ModuleList(\n",
       "          (0-1): 2 x BatchNorm1d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (linear_layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layer): Linear(in_features=1024, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add functionality to the training process using callback objects, such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. `JPlotLoss` - allows the interactive visualization of the loss curve when training inside a Jupyter Notebook, the `frequency` argument setting the refresh rate of the interactive widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  2. `StepLR` - implements a learning rate scheduler which decreases the learning rate in steps (after a number of iterations defined by `step_size` the learning rate is multiplied by `gamma`. Other types of learning rate schedulers can alternatively be used, such as `CyclicLR` or `ReduceLrOnPlateau`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  3. `SaveBestModel` - it enables the periodic saving of the weights of the neural network during training. After a number of iterations defined by the `freq` argument, the weights of the neural network are saved at the specified `path` if the current average loss (computed over the last `average` iterations) is lower than the loss for the previous save. The history of the losses and learning rate values is also saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the package is installed in editable mode, the default save path is relative to the repository directory (defined by `SAVED_MODELS_DIR`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_model_name = 'model_' + config_name + '.pt'\n",
    "save_path = str(SAVED_MODELS_DIR / save_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the callback objects together in a touple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "callbacks = (\n",
    "    JPlotLoss(frequency=10), \n",
    "    StepLR(step_size=5000, gamma=0.1, last_epoch=-1), \n",
    "    SaveBestModel(path=save_path, freq=100, average=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "The callbacks can also be initialized directly from the configuration file:\n",
    "`callbacks = get_callbacks_by_name(config_name)`\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The training process is initiated by invoking the `train` method of the trainer. This method accepts parameters including the previously defined tuple of callbacks, as well as the number of iterations (batches). Notably, a new batch of data is generated at each iteration, the training taking place in a \"one-epoch regime\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "trainer.train(num_batches=10, callbacks=callbacks)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train(num_batches=200, callbacks=callbacks, grad_accumulation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the history of the losses and learning rates can be accessed via `trainer.losses` and `trainer.lrs`. We can also find them together with the model state_dict in the saved dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'lrs', 'losses', 'prev_save', 'batch_num', 'best_loss'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(save_path).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    ":class: dropdown\n",
    "\n",
    "The saved weights can be loaded into a compatible neural network (`net`) as:\n",
    "\n",
    "```python\n",
    "saved_dict = torch.load(save_path)\n",
    "model_state_dict = saved_dict['model']\n",
    "net.load_state_dict(model_state_dict)\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing the YAML configuration for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the following we show how the YAML configuration file can be customized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{dropdown} Sample YAML configuration\n",
    "```yaml\n",
    "\n",
    "general:\n",
    "  name: val_sim_L2_q256_d300_r60_s25_bs4_budist_noise-poisson02\n",
    "  root_dir: null\n",
    "  \n",
    "dset:\n",
    "  prior_sampler:\n",
    "    cls: SubpriorParametricSampler\n",
    "    kwargs:\n",
    "      param_ranges:\n",
    "        thicknesses: [1., 300.]\n",
    "        roughnesses: [0., 60.]\n",
    "        slds: [0., 25.]\n",
    "      bound_width_ranges:\n",
    "        thicknesses: [1.0e-2, 300.]\n",
    "        roughnesses: [1.0e-2, 60.]\n",
    "        slds: [ 1.0e-2, 4.]\n",
    "      model_name: standard_model\n",
    "      max_num_layers: 2\n",
    "      constrained_roughness: true\n",
    "      max_thickness_share: 0.5\n",
    "      logdist: false\n",
    "      \n",
    "  q_generator:\n",
    "    cls: ConstantQ\n",
    "    kwargs:\n",
    "      q: [0.02, 0.3, 256]\n",
    "      remove_zero: false\n",
    "      fixed_zero: true\n",
    "      \n",
    "\n",
    "  intensity_noise: \n",
    "    cls: BasicExpIntensityNoise\n",
    "    kwargs:\n",
    "      relative_errors: [0.0, 0.2]\n",
    "      abs_errors: 0.0\n",
    "      consistent_rel_err: false\n",
    "      logdist: false\n",
    "      apply_shift: false\n",
    "      shift_range: [-0.001, 0.001]\n",
    "      apply_scaling: false\n",
    "      scale_range: [-0.001, 0.001]\n",
    "\n",
    "  q_noise:\n",
    "    cls: BasicQNoiseGenerator\n",
    "    kwargs:\n",
    "      shift_std: 1.0e-7\n",
    "      noise_std: [0., 1.0e-6]\n",
    "      \n",
    "  curves_scaler:\n",
    "    cls: LogAffineCurvesScaler\n",
    "    kwargs:\n",
    "      weight: 0.2 #0.2\n",
    "      bias: 1.0 #1.0\n",
    "      eps: 1.0e-10\n",
    "\n",
    "model:\n",
    "  encoder:\n",
    "    cls: SubPriorConvFCEncoder_V2\n",
    "    pretrained_name: null\n",
    "    kwargs:\n",
    "       hidden_dims: [32, 64, 128, 256, 512]\n",
    "       latent_dim: 8\n",
    "       conv_latent_dim: 128\n",
    "       avpool: 8\n",
    "       use_batch_norm: true\n",
    "       in_features: 256\n",
    "       prior_in_features: 16\n",
    "       hidden_features: 1024\n",
    "       num_blocks: 6  #3\n",
    "       fc_activation: 'gelu'\n",
    "       conv_activation: 'gelu' #'lrelu'\n",
    "       pass_bounds: false\n",
    "       pretrained_conv: null\n",
    "training:\n",
    "  train_with_q_input: False\n",
    "  num_iterations: 2000\n",
    "  batch_size: 4096\n",
    "  lr: 1.0e-4\n",
    "  update_tqdm_freq: 1\n",
    "  grad_accumulation_steps: 1\n",
    "  optimizer: AdamW\n",
    "\n",
    "  callbacks:\n",
    "    save_best_model:\n",
    "      enable: true\n",
    "      freq: 500\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `general` key, contains the following subkeys:\n",
    "\n",
    "- `name` - name used when saving the model\n",
    "- `root` - path to the root directory, defaults to the package directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```yaml\n",
    "general:\n",
    "  name: c1\n",
    "  root_dir: null\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```yaml\n",
    "\n",
    "model:\n",
    "  encoder:\n",
    "    cls: SubPriorConvFCEncoder_V2\n",
    "    pretrained_name: null\n",
    "    kwargs:\n",
    "       hidden_dims: [32, 64, 128, 256, 512]\n",
    "       latent_dim: 8\n",
    "       conv_latent_dim: 128\n",
    "       avpool: 8\n",
    "       use_batch_norm: true\n",
    "       in_features: 256\n",
    "       prior_in_features: 16\n",
    "       hidden_features: 1024\n",
    "       num_blocks: 6  #3\n",
    "       fc_activation: 'gelu'\n",
    "       conv_activation: 'gelu'\n",
    "       pass_bounds: false\n",
    "       pretrained_conv: null\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `training` key can be used to customize the training settings:\n",
    "\n",
    "- `num_iterations` - the total number of iterations the network is trained for\n",
    "- `batch_size` - the batch size (number of curves generated at each iteration)\n",
    "- `optimizer` - the used [Pytorch optimizer](https://pytorch.org/docs/stable/optim). Default is `AdamW`\n",
    "- `lr` - the initial learning rate\n",
    "- `grad_accumulation_steps` - if larger than 1, training is performed with gradient accumulation with the chosen number of steps\n",
    "\n",
    "- `update_tqdm_freq` - the frequency for updating the [tqdm progress bar](https://tqdm.github.io/)\n",
    "- `train_with_q_input` - must be set to `True` if the q-values are used as input (i.e. when the )\n",
    "- `callbacks` - (optional) the callback classes together with their arguments. Can also be defined directly as in the previous subsection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "\n",
    "training:\n",
    "  train_with_q_input: False\n",
    "  num_iterations: 2000\n",
    "  batch_size: 4096\n",
    "  lr: 1.0e-4\n",
    "  update_tqdm_freq: 1\n",
    "  grad_accumulation_steps: 1\n",
    "  optimizer: AdamW\n",
    "  callbacks:\n",
    "    save_best_model:\n",
    "      enable: true\n",
    "      freq: 500\n",
    "        \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
